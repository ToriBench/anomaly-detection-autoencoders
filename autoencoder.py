# -*- coding: utf-8 -*-
"""autoencoder_dnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cN1OE7I8seFicSEyL7Eq7-FqJVJnitl0
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
import tensorflow as tf
from tensorflow import keras
import datetime
import matplotlib.pyplot as plt
import cv2
import numpy as np
import pandas as pd
from keras import layers, losses
from keras.datasets import mnist
from keras.models import Model

!rm -rf ./logs/ 
(x_train, y_train), (x_test, y_test)  = keras.datasets.mnist.load_data()

x_train = x_train.astype('float32')/255
x_test = x_test.astype('float32')/255


train_filter_norm = np.where(y_train == 1)
test_filter_norm = np.where(y_test == 1)
test_filter_anom = np.where(y_test == 1)

norm_train_data, norm_train_labels = x_train[train_filter_norm], y_train[train_filter_norm]
norm_test_data, norm_test_labels = x_test[test_filter_norm], y_test[test_filter_norm]

anom_test_data = x_test[test_filter_anom]
print(norm_train_data)

autoencoder_dnn = tf.keras.Sequential([
      layers.Flatten(),
      layers.Dense(128, activation="relu"),
      layers.Dense(64, activation="relu"),
      layers.Dense(32, activation="relu"),
      layers.Dense(64, activation="relu"),
      layers.Dense(128, activation="relu"),
      layers.Dense(784, activation="sigmoid"),
      layers.Reshape((28, 28))])

autoencoder_dnn.compile(optimizer='adam', loss='binary_crossentropy')

log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

history = autoencoder_dnn.fit(norm_train_data, norm_train_data, 
          epochs=25, 
          validation_data=(norm_test_data, norm_test_data),
          callbacks=[tensorboard_callback])

autoencoder_cnn = tf.keras.Sequential([
  layers.Conv2D(32,(3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)),
  layers.MaxPooling2D((2, 2), padding='same'),
  layers.Conv2D(16, (3, 3), activation='relu', padding='same'),
  layers.MaxPooling2D((2, 2), padding='same'),
  layers.Conv2D(8, (3, 3), activation='relu', padding='same'),
  layers.UpSampling2D((2, 2)),
  layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
  layers.UpSampling2D((2, 2)),
  layers.Conv2D(1, (3, 3), activation='relu',padding='same'),
  layers.Reshape((28,28))])

autoencoder_cnn.compile(optimizer='adam', loss='binary_crossentropy')
autoencoder_cnn.summary()

log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")

tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

earlyStop_callback = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    min_delta=0,
    patience=0,
    verbose=0,
    mode='auto',
    baseline=None,
    restore_best_weights=False
)

history = autoencoder_cnn.fit(norm_train_data, norm_train_data, 
          epochs=16, 
          validation_data=(norm_test_data, norm_test_data),
          callbacks=[tensorboard_callback, earlyStop_callback])

autoencoder = autoencoder_dnn
reconstructions = autoencoder.predict(norm_test_data)
bce = tf.keras.losses.BinaryCrossentropy()
losses = np.zeros(len(norm_test_data))
print(bce(norm_test_data, reconstructions).numpy())
for i in range(len(norm_test_data)):
  losses[i]=bce(norm_test_data[i], reconstructions[i]).numpy()

print(np.mean(losses))

threshold = np.mean(losses) + 2 * np.std(losses)
print(threshold)

dataset = anom_test_data
reconstructions = autoencoder.predict(dataset)
bce = tf.keras.losses.BinaryCrossentropy()
anomalies = []
test_loss = np.zeros(dataset.shape[0])
for i in range(dataset.shape[0]-1):
  test_loss=bce(dataset[i], reconstructions[i]).numpy()
  if test_loss > threshold:
    anomalies.append(i)
counter = 0
print('Number of anomalies detected: ', len(anomalies))
print('Number of inputs: ', len(dataset))
print('Accuracy: ',(len(anomalies)/len(dataset)))

counter = 0
n = [x for x in range(len(dataset)) if x in anomalies]
plt.figure(figsize=(100, 4))
print(n)
for i in n:
  # display original
  ax = plt.subplot(2, len(n), counter + 1)
  plt.imshow(dataset[i])
  plt.title("original")
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)

  #display reconstruction
  ax = plt.subplot(2, len(n), len(n)+ 1 + counter)
  plt.imshow(reconstructions[i])
  plt.title("reconstructed")
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)
  counter +=1
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/fit